{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
    "\n",
    "### Karen Simonyan* & Andrew Zisserman +\n",
    "#### Visual Geometry Group, Department of Engineering Science, University of Oxford\n",
    "##### *current affiliation: GoogleDeepMind +current affiliation: University of Oxford and Google DeepMind\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why this paper?\n",
    "\n",
    "After suggestion of CNN, various CNN models have been created not by adjusting lots of hyper parameters, but because of their, researchers, own approach. For instance, there are many kinds of CNN model, like ResNet, VGG, Inception or LeNet. But I got to know that, especially, VGG has very simple structure and looks quite intuitive for users, at least for me. Hence, to get some intuition and mechanism of how they are created, I chose this research paper. This paper was issued in 2014 and by using this model, they won first and second places in the localization and classification tracks respectively in ImageNet Challenge 2014.\n",
    "\n",
    "### Contents of Paper\n",
    "\n",
    "##### Structure of model\n",
    "\n",
    "Main point of this paper is also as simple as their model, VGG. They talked about its method, approach and their decision process during their work. First of all let’s get into its structure. \n",
    "Before training, they planned to design 6 networks according to their own method. All of models are rooted in same frame. Networks have difference only in its depth. Then, what is the frame? All they have 224 * 224 RGB inputs, 3 * 3 filter size (sometime, 1 * 1, talk about later), 1 stride and padding for preservation. Number of filters start from 64 and increase to 512. All increase of number of filters are done only after max-pooling. The depth of their 6 networks is determined by the number of convolutional layers before max-pooling. Max-pooling is done with 2 * 2 and 2 stride. After all convolutional layers, there are 4096-4096-1000 fully connected layers, which is followed by softmax layer to classify. Lastly, all non-linear functions are ReLu. We can find its simplicity of their networks. It is really intuitive and easily understandable. And I will not mention about detail hyperparameters of their networks like momentum, iteration number or drop-out. It is not content which is mainly discussed in this paper. \n",
    "\n",
    "##### Result\n",
    "\n",
    "Result was quite impressive. Final model was ensemble of their networks that performed well in test. They showed surpassing performance recoding 23.7%, 6.8% and 6.8% in top-1 validation error, top-5 validation error and top-5 test error, which was measurement of ILSVRC classification.\n",
    "\n",
    "### Feature of thier model\n",
    "##### 3 * 3, 1 * 1 filter\n",
    "\n",
    "VGG’s feature can be said, its simplicity. For simplicity, only depth is used. And for adjusting depth, only 3 * 3 filter and 1 * 1 filter are used. Unlike other models using 5 * 5, 7 * 7 or even 11 * 11, they mentioned that they mainly use 3 * 3 filter. The reason why they use 3 * 3 is because it has some advantages for classifying. First, they said that, to make deep model, to get simplicity, 3 * 3 filter is needed. Using bigger size of filter rather than 3 * 3, the width and height would shrink rapidly and it makes impossible to build deep networks. \n",
    "\n",
    "----\n",
    "\n",
    "Secondly, they can reduce number of parameters effectively to prevent it from overfitting. Let’s compare to 7 * 7 filter. Then, number of parameters would be (7 * 7 * c) * 7, 49*c^2, rather than 27*c^2. This can be treated as forcing 7 * 7 filters to be regularized. \n",
    "\n",
    "----\n",
    "\n",
    "Third, its ability for discrimination. We can think 1 of 7 * 7 filter has same result of 3 of 3 * 3 filter. And by doing so, they think they can get more discriminative models. These are why they decided to use 3 * 3 filter.\n",
    "\n",
    "----\n",
    "\n",
    "Then, what about 1 * 1 filter. It’s one of trendy method to make model has more non-linearity which is key point of neural networks. It is proved and used in lots of other research papers. It can give more non-linearity not affecting its structure.\n",
    "\n",
    "\n",
    "### Interesting points\n",
    "\n",
    "##### Back to the basic\n",
    "\n",
    "We have seen all of process to improve networks. What I felt after understanding this research paper is “Back to the basic”. Machine learning, which became some kind of magic in their performance these days, it is not magic though, is rooted on statistics. Deep learning, which is the most powerful algorithm for image or speech, is rooted on machine learning. What I want to say is that the way many fancy neural networks are invented is, after all, problem with basic elements of them like, non-linearity, overfitting or loss. All of ideas just come from very, very, foundation. When I worked on a project, I have been encountered lots of time that I have to make a decision. There are lots of hyperparameters or things about feature engineering. Number of cases is getting bigger like one in Go. What could I do then? I think I got to know a bit how to deal with those problem. Anyway, it’s impossible to do all of cases. Plan and do fast to get result, even if it would be rough and not good. ML project usually consists of process, Idea -> Code -> Experiments. Sticking to on circulation, which might be not effective, doesn’t look productive. In this paper, all of the decision have been made by foundational idea. And I think, with this decision making, circulation of machine learning project could be more efficient and effective.\n",
    "\n",
    "##### Community\n",
    "\n",
    "Another feature of how they build networks is to use community, and so other research papers does. They use community, idea of other teams, very actively. They don’t seem to try to hide all of ideas they get, they even look very willing to share idea. I think it is another factor that can make circulation faster. Like ensemble, it is more likely to find effective idea when many teams, many people participate in the community. And I could find another lacking point in my previous project. Back then, we were not good at machine learning and we didn’t even know that feature engineering is very important when we try to apply machine learning algorithm. And even after we notice the importance, we encountered difficulty to find how to deal with that. I think we should have referred to meaningful idea of others. There have been lots of community we can refer like Kaggle, research paper or Github. \n",
    "\n",
    "\n",
    "### Limitation of me\n",
    "\n",
    "##### LRN? other augmentation method?\n",
    "\n",
    "Other than content about VGG’s structure, there are measurements, how to crop or LRN which is not keyword familiar to me. First, what is crop? Crop is used for data augmentation. To improve performance, especially to reduce the gab between dev error and training error, we need more data but we are not always able to get new data. Hence, there is crop. It crops some input data to augment. And in this paper, I’ve seen new, and various method how to use crop. It might seem to refer paper of Krizhevsky, it was first time for me to see those kinds of method. They said they randomly cropped from rescaled training images (one crop per image per SGD iteration). \n",
    "\n",
    "\n",
    "And they tried to use LRN (Local Response Normalization). It is known that LRN is used to reduce generalization error, normalizing specific area according to specific rule defined on that specific area. But It is not used these days, because it doesn’t improve well. Also, in this paper, they decided not to use this method after seeing there isn’t any improvement. \n",
    "\n",
    "\n",
    "I was interested in these new keywords for me. They seemed that they have played important role or have had played and there is common feature. All of them are referred in paper of Krizhevsky. So, I am going to review Krizhevsky’s research papers after this\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
